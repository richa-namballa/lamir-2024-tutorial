{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53969bfa-7815-4a84-98c3-b5f870a75e6f",
      "metadata": {
        "id": "53969bfa-7815-4a84-98c3-b5f870a75e6f"
      },
      "source": [
        "# Musical Source Separation with Limited Data\n",
        "## LAMIR 2024 Hackathon\n",
        "\n",
        "Authored by Richa Namballa\n",
        "\n",
        "Based on the late-breaking demo:\n",
        "> Namballa, R., Morais, G., \\& Fuentes, M. (2024). \"Musical Source Separation of Brazilian Percussion.\" In _Extended Abstracts for the Late-Breaking Demo Session of the 25th International Society for Music Information Retrieval Conference_."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Musical source separation (MSS)** is a central task of music information retrieval (MIR) which aims to “de-mix” audio into its corresponding instrument stems. It has applications in both the research and production of music by allowing the analysis and reuse of the stems.\n",
        "\n",
        "For a more detailed introduction on the task of the source separation itself, please refer to the [**Open Source Tools & Data for Music Source Separation**](https://source-separation.github.io/tutorial/landing.html) tutorial writted by Ethan Manilow, Prem Seetharaman, and Justin Salamon.\n",
        "\n",
        "Some source separation models, such as [**Demucs**](https://github.com/adefossez/demucs), have reached a state-of-the-art level in their ability to celebrate musical mixtures into four stems: _drums_, _bass_, _vocals_ and _other_. However, most source separation systems are trained to process Western instruments only, precluding their application to more culturally-diverse music.\n",
        "\n",
        "### Datasets\n",
        "\n",
        "There are many source separation datasets available to use for modeling training such as, [**Slakh2100**](http://www.slakh.com/). One of the most popular 4-stem MSS datasets is [**MUSDB18**](https://sigsep.github.io/datasets/musdb.html#musdb18-compressed-stems), which contains 150 full length audio tracks. Even those datasets which advertise a larger variety of stems, such as [**MoisesDB**](https://github.com/moises-ai/moises-db), are focused on Eurocentric instruments. Creating new MSS datasets is challenging due to the time and monetary cost required to record and mix high-quality stems, thus the lack of diversity in instrumentation is expected. Prior to investing significant resources into constructing new datasets, we investigate the feasibility of building an MSS system by artificially creating mixtures featuring an existing non-Western dataset.\n",
        "\n",
        "We choose to use the [**Brazilian Rhythmic Instruments Dataset**](https://zenodo.org/records/14051323), a dataset typically used in the context of beat tracking. For this demo, we elected to set the _surdo_ as our target source to separate from the mixture. The surdo is a large tom-like drum which plays a distinctive pattern repeated throughout the piece. This trait, plus its distinctive low-pitched timbre, makes it an easier target compared to the other percussion instruments."
      ],
      "metadata": {
        "id": "n2gKxlzXsZBJ"
      },
      "id": "n2gKxlzXsZBJ"
    },
    {
      "cell_type": "markdown",
      "id": "63963acb-9a16-4ad7-a1e8-840a42b91f78",
      "metadata": {
        "id": "63963acb-9a16-4ad7-a1e8-840a42b91f78"
      },
      "source": [
        "#### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9089081e-7753-4532-8590-2f559321637f",
      "metadata": {
        "id": "9089081e-7753-4532-8590-2f559321637f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from spectrogram import generate_spectrograms\n",
        "from unet import UNet\n",
        "from dataset import SeparationDataset\n",
        "from utils import plot_loss, Spec2Audio\n",
        "from separate import separate\n",
        "\n",
        "from IPython.display import Audio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9acf8c3f-cba5-49a8-b1de-2d5341f7aa8d",
      "metadata": {
        "id": "9acf8c3f-cba5-49a8-b1de-2d5341f7aa8d"
      },
      "source": [
        "### SynBRID Dataset\n",
        "\n",
        "For this task, we artificially generated our own mixtures by combining BRID solo tracks to create `syn_brid`. In total, we generated 100 mixtures for training, 10 for validation, and 30 for testing. For each song, we provide the mix (`mixture.wav`) and the surdo stem (`surdo.wav`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aa84b66-5b9d-48f6-9196-0fffe0756263",
      "metadata": {
        "id": "5aa84b66-5b9d-48f6-9196-0fffe0756263"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "# extract .wav files from syn_brid.tar.gz\n",
        "# open file\n",
        "if not os.path.isdir('syn_brid/'):\n",
        "    with tarfile.open('syn_brid.tar.gz') as f:\n",
        "        # extract compressed files\n",
        "        f.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1070e6a2-52c4-4600-b663-b8222f0843cd",
      "metadata": {
        "id": "1070e6a2-52c4-4600-b663-b8222f0843cd"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "The source separation model is built using the magnitude spectrograms of the mixture and stem. For computational efficiency, we use a low sample rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e21ea98d-3365-43e8-b440-3dfc696e0c92",
      "metadata": {
        "id": "e21ea98d-3365-43e8-b440-3dfc696e0c92"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 8192\n",
        "FFT_SIZE = 1024\n",
        "HOP_SIZE = 768\n",
        "PATCH_SIZE = 128\n",
        "\n",
        "TARGET_SOURCE = \"surdo\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29722957-4b3f-4f4e-9a1b-726610a37a60",
      "metadata": {
        "id": "29722957-4b3f-4f4e-9a1b-726610a37a60"
      },
      "outputs": [],
      "source": [
        "# generate the spectrograms for each fold of the dataset\n",
        "print(\"\\n>>> TRAINING DATA <<<\")\n",
        "generate_spectrograms('./syn_brid/train', './spec/train', TARGET_SOURCE, SAMPLE_RATE, FFT_SIZE, HOP_SIZE)\n",
        "print(\"\\n>>> VALIDATION DATA <<<\")\n",
        "generate_spectrograms('./syn_brid/val', './spec/val', TARGET_SOURCE, SAMPLE_RATE, FFT_SIZE, HOP_SIZE)\n",
        "print(\"\\n>>> TESTING DATA <<<\")\n",
        "generate_spectrograms('./syn_brid/test', './spec/test', TARGET_SOURCE, SAMPLE_RATE, FFT_SIZE, HOP_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7c4dc55-37be-46dd-92ca-b101d65405b3",
      "metadata": {
        "id": "f7c4dc55-37be-46dd-92ca-b101d65405b3"
      },
      "source": [
        "### Test the Base Model\n",
        "\n",
        "We pretrained the source separation model on the _bass_ stem from the MUSDB dataset with a learning rate of `1e-4` and 1000 epochs. Let's listen to what it sounds like if we use the bass (base) model directly on our SynBRID mixtures to try and separate the surdo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec39671-9ee8-4b2d-ac75-cf0baa2fbec7",
      "metadata": {
        "id": "0ec39671-9ee8-4b2d-ac75-cf0baa2fbec7"
      },
      "outputs": [],
      "source": [
        "DEVICE_TYPE = \"cuda\"\n",
        "IN_CHANNELS = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f794c9e-d3a6-4cff-86e1-7fa342cd1fa9",
      "metadata": {
        "id": "7f794c9e-d3a6-4cff-86e1-7fa342cd1fa9"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(DEVICE_TYPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cefb6f-9fbc-4325-8bce-dfc04f3f040f",
      "metadata": {
        "id": "59cefb6f-9fbc-4325-8bce-dfc04f3f040f"
      },
      "outputs": [],
      "source": [
        "print(\"Loading base model...\")\n",
        "# initialize model\n",
        "base_model = UNet(IN_CHANNELS)\n",
        "# load weights\n",
        "bass_weights = torch.load('best_weights_bass.pth', weights_only=True)\n",
        "base_model.load_state_dict(bass_weights)\n",
        "print(\"Base model loaded succesfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f086055c-9995-4034-ae9c-20386ddd6d1f",
      "metadata": {
        "id": "f086055c-9995-4034-ae9c-20386ddd6d1f"
      },
      "outputs": [],
      "source": [
        "# set a seed to choose a random mixture from the test set\n",
        "test_seed = 14\n",
        "random.seed(test_seed)\n",
        "files = [f for f in os.listdir('./syn_brid/test') if not f.startswith('.')]\n",
        "test_mixture = os.path.join('syn_brid', 'test', random.choice(files), 'mixture.wav')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f3cdede-a6ef-462d-bcd0-efd81239e6d6",
      "metadata": {
        "id": "9f3cdede-a6ef-462d-bcd0-efd81239e6d6"
      },
      "outputs": [],
      "source": [
        "stem = separate(test_mixture, base_model, DEVICE, FFT_SIZE, HOP_SIZE, SAMPLE_RATE, PATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029dfa20-0e9a-4dac-93e9-98f0ba402cc1",
      "metadata": {
        "id": "029dfa20-0e9a-4dac-93e9-98f0ba402cc1"
      },
      "outputs": [],
      "source": [
        "Audio(stem, rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hmmm not too great... can we do better?"
      ],
      "metadata": {
        "id": "xzhr12Qyz7Gl"
      },
      "id": "xzhr12Qyz7Gl"
    },
    {
      "cell_type": "markdown",
      "id": "5a377f13-6c0b-4b78-9918-507f59a16f1f",
      "metadata": {
        "id": "5a377f13-6c0b-4b78-9918-507f59a16f1f"
      },
      "source": [
        "### Fine-tuning the Base Model\n",
        "\n",
        "_Fine-tuning_ is a _transfer learning_ method where \"tune\" (or adjust) the weights of the pretrained model to work on new data. When you fine-tune, you have the option of continuing to train all of the parameters on new data or \"freeze\" the earlier layers so that the initial feature extraction remains the same.\n",
        "\n",
        "We have provided some basic code that you use as a starting point to improve the surdo separation model. In the dataloader, we have provided an argument `pct_files` which represents the percentage of each data subset to use. For example, if you set `pct_files=0.5` in the `train_dataset`, you will use 50 spectrograms from the training set to fine-tune the model.\n",
        "\n",
        "**Task**: Your task is to see how much you can improve the surdo separation model with the smallest amount of data (i.e., the lowest value of `pct_files` for `train_dataset`). Feel free to experiment and make modificiations to the training code and other scripts.\n",
        "\n",
        "Some suggestions to improve your model:\n",
        "* Test different values of the hyperparameters (the learning rate, number of epochs, etc.)\n",
        "* Add more fine-tuning layers.\n",
        "* Try freezing the earlier layers of the model to only fine-tune the parameters of the later layers.\n",
        "* Use different `random_state` seeds.\n",
        "* Be creative!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8d12fce-4734-4138-b9eb-225b44d8443a",
      "metadata": {
        "id": "f8d12fce-4734-4138-b9eb-225b44d8443a"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "NUM_WORKERS = 2\n",
        "LEARNING_RATE = 1e-6\n",
        "NUM_EPOCHS = 500\n",
        "PERCENT_TRAIN = 1.\n",
        "PERCENT_VAL = 1."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80a7de63-2873-49b9-9f3f-594d9f6c8e80",
      "metadata": {
        "id": "80a7de63-2873-49b9-9f3f-594d9f6c8e80"
      },
      "source": [
        "#### Set-Up Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "CmYWiyEymLnk"
      },
      "id": "CmYWiyEymLnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd6119fb-9d0d-4ebf-b0cd-422b279bfe13",
      "metadata": {
        "id": "fd6119fb-9d0d-4ebf-b0cd-422b279bfe13"
      },
      "outputs": [],
      "source": [
        "train_dataset = SeparationDataset('./spec/train/', TARGET_SOURCE,\n",
        "                                  pct_files=PERCENT_TRAIN,\n",
        "                                  patch_size=PATCH_SIZE, random_state=RANDOM_STATE)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                              num_workers=NUM_WORKERS, shuffle=True)\n",
        "\n",
        "val_dataset = SeparationDataset('./spec/val/', TARGET_SOURCE,\n",
        "                                pct_files=PERCENT_VAL,\n",
        "                                patch_size=PATCH_SIZE)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
        "                            num_workers=NUM_WORKERS, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f44bffc-2217-4bea-86c1-acf518302d8a",
      "metadata": {
        "id": "7f44bffc-2217-4bea-86c1-acf518302d8a"
      },
      "source": [
        "### Training\n",
        "\n",
        "Run this code to fine-tune the model. Make sure that you are connected to a GPU runtime to increase the speed of your training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f88904e-450c-425d-a1d4-ba5c7b9de2ad",
      "metadata": {
        "id": "3f88904e-450c-425d-a1d4-ba5c7b9de2ad"
      },
      "outputs": [],
      "source": [
        "# get timestamp for saving checkpoints and history\n",
        "t_stamp = datetime.now().strftime(\"%y%m%d_%I%M%S%p\")\n",
        "\n",
        "# create checkpoint directory\n",
        "os.makedirs('./checkpoint', exist_ok=True)\n",
        "\n",
        "model_name = f\"{t_stamp}_FT_{TARGET_SOURCE}.pth\"\n",
        "history_name = f\"{t_stamp}_FT_history.pkl\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "664636d5-7679-4079-9f10-29127a807b8d",
      "metadata": {
        "id": "664636d5-7679-4079-9f10-29127a807b8d"
      },
      "outputs": [],
      "source": [
        "# load the bass base model weights to have a starting point for training\n",
        "print(\"Loading base model for fine-tuning...\")\n",
        "ft_model = UNet(IN_CHANNELS)\n",
        "ft_model.load_state_dict(torch.load('best_weights_bass.pth', weights_only=True))\n",
        "print(\"Base model ready for fine-tuning!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fine-tune all model parameters\n",
        "for param in ft_model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "YYyjf4YiNafM"
      },
      "id": "YYyjf4YiNafM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3701927-8966-44de-a8e7-6ccf0c2c8bd7",
      "metadata": {
        "id": "e3701927-8966-44de-a8e7-6ccf0c2c8bd7"
      },
      "outputs": [],
      "source": [
        "# send model to GPU if available\n",
        "ft_model = ft_model.to(DEVICE)\n",
        "\n",
        "# save best model\n",
        "best_model = None\n",
        "best_val_loss = 1e6\n",
        "\n",
        "# initialize training\n",
        "# mean absolute error loss\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(ft_model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# enable mixed precision\n",
        "scaler = torch.GradScaler(DEVICE_TYPE)\n",
        "\n",
        "# save history of metrics\n",
        "history = {'train_loss': [], 'val_loss': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766cb5a3-f225-4449-b6e6-7a59aade1e77",
      "metadata": {
        "scrolled": true,
        "id": "766cb5a3-f225-4449-b6e6-7a59aade1e77"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # loss values within epoch\n",
        "    train_loss_epoch, val_loss_epoch = [], []\n",
        "\n",
        "    # TRAINING\n",
        "    # enable training\n",
        "    ft_model.train()\n",
        "\n",
        "    # progress bar\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    pbar.set_description(\"Training\")\n",
        "\n",
        "    for idx, (mix, stem, phase) in enumerate(pbar):\n",
        "        # send data to device\n",
        "        mix = mix.to(DEVICE)\n",
        "        stem = stem.to(DEVICE)\n",
        "\n",
        "        # autocast data type\n",
        "        with torch.autocast(device_type=DEVICE_TYPE, dtype=torch.float32):\n",
        "            output = ft_model(mix)\n",
        "            loss = criterion(output, stem)\n",
        "\n",
        "        train_loss_epoch.append(loss.item())\n",
        "\n",
        "        pbar.set_postfix({\"Loss\": loss.item()}, refresh=True)\n",
        "\n",
        "        # backward\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "    # compute avg training loss for this epoch\n",
        "    history['train_loss'].append(np.mean(train_loss_epoch))\n",
        "\n",
        "    # VALIDATION\n",
        "    ft_model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for mix, stem, phase in val_dataloader:\n",
        "            # send data to device\n",
        "            mix = mix.to(DEVICE)\n",
        "            stem = stem.to(DEVICE)\n",
        "\n",
        "            # forward\n",
        "            preds = ft_model(mix)\n",
        "            loss = criterion(preds, stem)\n",
        "\n",
        "            val_loss_epoch.append(loss.item())\n",
        "\n",
        "    # compute avg validation loss for this epoch\n",
        "    history['val_loss'].append(np.mean(val_loss_epoch))\n",
        "\n",
        "    # log summary for epoch\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}: \" +\n",
        "             f\"Training Loss: {history['train_loss'][-1]:.6f}, \" +\n",
        "             f\"Validation Loss: {history['val_loss'][-1]:.6f}\\n\")\n",
        "\n",
        "    # check if the model improved on the validation dataset\n",
        "    if history['val_loss'][-1] < best_val_loss:\n",
        "        best_model = ft_model\n",
        "        torch.save(best_model.state_dict(),\n",
        "                   os.path.join('checkpoint/', model_name))\n",
        "        best_val_loss = history['val_loss'][-1]\n",
        "\n",
        "print(\"Training completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9ed3317-d13e-47ab-9fe8-33e9f25bfcac",
      "metadata": {
        "id": "a9ed3317-d13e-47ab-9fe8-33e9f25bfcac"
      },
      "outputs": [],
      "source": [
        "# save training history\n",
        "with open(os.path.join('./checkpoint/', history_name), 'wb') as f:\n",
        "    pickle.dump(history, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c48901c-c68a-46d9-bae2-28856c04fbc7",
      "metadata": {
        "id": "1c48901c-c68a-46d9-bae2-28856c04fbc7"
      },
      "outputs": [],
      "source": [
        "# plot loss curve\n",
        "plot_loss(history, save_path=f'./checkpoint/{t_stamp}_loss.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbbb411-0087-4dcc-9dc2-9ea215296663",
      "metadata": {
        "id": "acbbb411-0087-4dcc-9dc2-9ea215296663"
      },
      "source": [
        "### Test the Fine-tuned Model\n",
        "\n",
        "Load the fine-tuned model and run it on the same test mixture from before.\n",
        "\n",
        "We also recommend that you run the model on the entire test set and compute traditional MSS performance metrics, such as [Source-to-Distortion Ratio (SDR)](https://lightning.ai/docs/torchmetrics/stable/audio/signal_distortion_ratio.html). Make sure that you compute the SDR for the base model on the surdo test set as well. A higher SDR is interpreted as better separation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54adb90f-4676-4fad-84ca-8c596ac99cd1",
      "metadata": {
        "id": "54adb90f-4676-4fad-84ca-8c596ac99cd1"
      },
      "outputs": [],
      "source": [
        "ckpt_path = f'./checkpoint/{model_name}'\n",
        "\n",
        "print(\"Loading fine-tuned model...\")\n",
        "# initialize model\n",
        "ft_model = UNet(IN_CHANNELS)\n",
        "# load weights\n",
        "surdo_weights = torch.load(ckpt_path, weights_only=True)\n",
        "base_model.load_state_dict(surdo_weights)\n",
        "print(\"Fine-tuned model loaded succesfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6522e0ad-4119-44b4-b290-a5950843d715",
      "metadata": {
        "id": "6522e0ad-4119-44b4-b290-a5950843d715"
      },
      "outputs": [],
      "source": [
        "surdo_stem = separate(test_mixture, ft_model, DEVICE, FFT_SIZE, HOP_SIZE, SAMPLE_RATE, PATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dab909b-7461-4b18-a411-a01787db792a",
      "metadata": {
        "id": "9dab909b-7461-4b18-a411-a01787db792a"
      },
      "outputs": [],
      "source": [
        "Audio(surdo_stem, rate=SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torchmetrics\n",
        "# from torchmetrics.audio import SignalDistortionRatio\n",
        "# ..."
      ],
      "metadata": {
        "id": "4q1kDR3vCw4I"
      },
      "id": "4q1kDR3vCw4I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}